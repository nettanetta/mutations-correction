{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ems/elsc-labs/habib-n/yuval.rom/miniconda3/envs/mutation_correction_env_1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_scheduler, BertForMaskedLM, DataCollatorWithPadding\n",
    "from transformers.models.bert.configuration_bert import BertConfig\n",
    "from dnabert_for_token_classification import BertForTokenClassification\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "from data_handling_for_MLM import MutationDetectionDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "Tokenizing the normal and mutated data,\n",
    "Marking what token has been changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_m = '/ems/elsc-labs/habib-n/yuval.rom/school/ANLP/final_project/Mutation-Simulator/data/sample_data/data_m.fa'\n",
    "fasta_t = '/ems/elsc-labs/habib-n/yuval.rom/school/ANLP/final_project/Mutation-Simulator/data/sample_data/data.fa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ems/elsc-labs/habib-n/yuval.rom/miniconda3/envs/mutation_correction_env_1/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "config = BertConfig.from_pretrained(\"zhihan1996/DNABERT-2-117M\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 2]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()\n",
    "tokenizer('[PAD]').input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tensor([   1,    4, 1004,   67,   36,  726,  528, 1104,  319,  746,  296,   28,\n",
      "          75, 1507,   55,  362,  123,  130,   82,  443,  184, 2063, 2169,  161,\n",
      "          83,  180,    4,    4,  588,  126,  545,    2])\n",
      "tensor([   1,  153, 1004,   67,   36,  726,  528, 1104,  319,  746,  296,   28,\n",
      "          75, 1507,   55,  362,  123,  130,   82,  443,  184, 2063, 2169,  161,\n",
      "          83,  180, 1719,   54,  588,  126,  545,    2])\n",
      "True\n",
      "----------------\n",
      "\n",
      "tensor([   1,   30,  126,  545,   66,  374, 1602,  283, 1108,  152,  793,  128,\n",
      "         245,   61,  208, 3056,  552,  635,   99,  819,   42,  558,  283,   65,\n",
      "         232,  204,   32,    4,   75, 3715,  151,    2])\n",
      "tensor([   1,   30,  126,  545,   66,  374, 1602,  283, 1108,  152,  793,  128,\n",
      "         245,   61,  208, 3056,  552,  635,   99,  819,   42,  558,  283,   65,\n",
      "         232,  204,   32,  289,   75, 3715,  151,    2])\n",
      "True\n",
      "----------------\n",
      "\n",
      "tensor([   1,    5,   23,   78,  479, 3270,   32,    4,    4,    4,  158,   70,\n",
      "         886,  232, 1115,  560,   74,   72,  425, 1432,   41,  148,   83,  708,\n",
      "         139,   28,  240,   74,  151,   15,  304,    2])\n",
      "tensor([   1,    5,   23,   78,  479, 3270,   32, 2171,  253,   22,  158,   70,\n",
      "         886,  232, 1115,  560,   74,   72,  425, 1432,   41,  148,   83,  708,\n",
      "         139,   28,  240,   74,  151,   15,  304,    2])\n",
      "True\n",
      "----------------\n",
      "\n",
      "tensor([   1,   42,  329,   57,   65,  720,   33,  181,  351, 1587,  876,  102,\n",
      "          65,  651, 1005,    7,    2])\n",
      "tensor([   1,   42,  329,   57,   65,  720,   33,  181,  351, 1587,  876,  102,\n",
      "          65,  651, 1005,    7,    2])\n",
      "True\n",
      "----------------\n",
      "\n",
      "tensor([   1, 2089,   72,   78,   65,  134,  161,   13,  212,   67,    4,    4,\n",
      "           4,    4, 1281,  178,   55, 2306, 1176,  209,   90,   86,   26,  278,\n",
      "          62,   64,  102,   56,  143,   15,  156,    2])\n",
      "tensor([   1, 2089,   72,   78,   65,  134,  161,   13,  212,   67,  136,   36,\n",
      "         114, 2617, 1281,  178,   55, 2306, 1176,  209,   90,   86,   26,  278,\n",
      "          62,   64,  102,   56,  143,   15,  156,    2])\n",
      "True\n",
      "----------------\n",
      "torch.int64\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "mutation_dataset = MutationDetectionDataset(fasta_m, fasta_t, tokenizer, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   1,  250, 1004,   67,   36,  726,  528, 1104,  319,  746,  296,   28,\n",
      "          75, 1507,   55,  362,  123,  130,   82,  443,  184, 2063, 2169,  161,\n",
      "          83,  180,   90, 1898,  588,  126,  545,   66,  374, 1602,  283, 1108,\n",
      "         152,  645,  215, 3373,  678, 2045,  556, 1176,  727,   97,  173,  448,\n",
      "        1227,  486,   48,  220,   65,   20,  164,  268,   27,  283,  104, 1184,\n",
      "          73, 3532,  245,   61,  208, 3056,  552,  635,   99,  819,   42,  558,\n",
      "         283,   65,  232,  204,   32,  289,   75, 3715,  151,  987, 1435,  226,\n",
      "          33,  411,  149, 3654,  494,  163, 1321,   53, 2975,  112,  131, 1069,\n",
      "         776,  937, 3920,  108, 1957,   80,  716, 3833,  177,  100,  118,   48,\n",
      "         941, 3115,  519, 1624,  107,   89,  586,   31,  181,   51, 1127, 2293,\n",
      "         448, 3462, 3454,  942,  307,   82, 2491,   50, 1431,  116,   28,  347,\n",
      "         220,   95,  366,  637,   68, 1886,  257,   53,  769,  210, 1795,   19,\n",
      "         402,  232,  586, 1889,  403, 3671,  341, 2793,   74,  220,   53, 3667,\n",
      "        1702, 1490, 1767,  215,  108, 2046,  163, 2034,  567,   44,  112,   24,\n",
      "         621,  134, 2288,   53,   57,   26,  166,  576,  281,  177,   89,   26,\n",
      "          61,  822,  264,  112,  241,  261, 2293,  144,   54, 1403,  177,  123,\n",
      "          36,   71,  382,   29,  432,   15,  363,   32,  165,  776,   86,  177,\n",
      "          95,   64, 3371,  952,  174,  390,   95,   54, 1023,  232,   88,  228,\n",
      "         133,   77,   62,  205,  308,  115,  102,   37,   41,  379,   20,  330,\n",
      "          26,  249,  460,  599, 2662,   92, 2409,  418,   95, 1763,  257,  456,\n",
      "          74, 2384,  241,   79,   74,  204, 2384,  192,   73,   88,  121, 1883,\n",
      "         103,  283,   26,  144,   41, 3901,  209,   78, 1355,   34,  487,  124,\n",
      "         415,  100, 1585,  888,  210,  477, 2804, 1539,  731,  552,   32,  800,\n",
      "         536,   55,  102,   30, 1621,  357, 3087,   30,  329,  186,  209,  837,\n",
      "         428,   62,  828, 2523,  232,   40,  418,  552, 1708,   45,   53,   22,\n",
      "        1922,   60,  452,   95,  452,   99,  993,   53, 2041,  112,   67,   59,\n",
      "         102,   50,  134,  111,  177,   44,  105,   50,  130, 1269,  476,  146,\n",
      "         101,   54,    2])\n",
      "x tensor([   1,    4, 1004,   67,   36,  726,  528, 1104,  319,  746,  296,   28,\n",
      "          75, 1507,   55,  362,  123,  130,   82,  443,  184, 2063, 2169,  161,\n",
      "          83,  180,    4,    4,  588,  126,  545,   66,  374, 1602,  283, 1108,\n",
      "         152,  645,  215,    4,  678, 2045,  556, 1176,  727,   97,  173,  448,\n",
      "        1227,  486,   48,  220,   65,   20,    4,  268,   27,  283,  104, 1184,\n",
      "          73, 3532,  245,   61,  208, 3056,  552,  635,   99,  819,   42,  558,\n",
      "         283,   65,  232,  204,   32,  289,   75, 3715,  151,  987, 1435,  226,\n",
      "          33,  411,  149, 3654,  494,  163, 1321,   53, 2975,  112,  131, 1069,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4, 1127, 2293,\n",
      "         448, 3462, 3454,  942,  307,   82, 2491,   50, 1431,  116,   28,  347,\n",
      "         220,   95,  366,  637,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,  776,   86,  177,\n",
      "          95,   64, 3371,    4,  174,  390,   95,   54, 1023,  232,   88,  228,\n",
      "         133,   77,   62,  205,  308,  115,  102,   37,   41,  379,   20,  330,\n",
      "          26,  249,  460,  599, 2662,   92,    4,    4,   95, 1763,  257,  456,\n",
      "          74, 2384,  241,   79,   74,  204, 2384,  192,   73,   88,  121, 1883,\n",
      "         103,    4,    4,  144,   41, 3901,  209,   78, 1355,   34,  487,  124,\n",
      "         415,  100, 1585,  888,  210,  477, 2804, 1539,  731,  552,   32,  800,\n",
      "         536,    4,  102,   30, 1621,  357, 3087,   30,  329,  186,  209,  837,\n",
      "         428,   62,  828, 2523,  232,   40,  418,  552, 1708,   45,   53,   22,\n",
      "        1922,   60,  452,   95,  452,   99,  993,   53, 2041,  112,   67,   59,\n",
      "         102,   50,  134,  111,  177,   44,  105,   50,  130, 1269,  476,  146,\n",
      "         101,   54,    2])\n",
      "y tensor([   1,  153, 1004,   67,   36,  726,  528, 1104,  319,  746,  296,   28,\n",
      "          75, 1507,   55,  362,  123,  130,   82,  443,  184, 2063, 2169,  161,\n",
      "          83,  180, 1719,   54,  588,  126,  545,   66,  374, 1602,  283, 1108,\n",
      "         152,  645,  215, 1347,  678, 2045,  556, 1176,  727,   97,  173,  448,\n",
      "        1227,  486,   48,  220,   65,   20,  150,  268,   27,  283,  104, 1184,\n",
      "          73, 3532,  245,   61,  208, 3056,  552,  635,   99,  819,   42,  558,\n",
      "         283,   65,  232,  204,   32,  289,   75, 3715,  151,  987, 1435,  226,\n",
      "          33,  411,  149, 3654,  494,  163, 1321,   53, 2975,  112,  131, 1069,\n",
      "          28,  619,  285,   78,  108, 1957,   80,  716, 3833,  177,  100,  118,\n",
      "          48,  941, 3115,  519, 1624,  107,   89,  586,  535,  173, 1127, 2293,\n",
      "         448, 3462, 3454,  942,  307,   82, 2491,   50, 1431,  116,   28,  347,\n",
      "         220,   95,  366,  637,   35,  136,   47,  257,   53,  769,  210, 1795,\n",
      "          19,  402,  232,  586, 1889,  403, 3671,  341, 2793,   74,  220,   53,\n",
      "        3667, 1702, 1490, 1767,  215,  108, 2046,  163, 2034,  567,   44,  112,\n",
      "          24,  621,  134, 2288,   53,   57,   26,  166,  348,  281,  177,   89,\n",
      "          26,   61,  822,  567,   32,  241,  261, 2293,  144,   54, 1403,  177,\n",
      "         123,   36,   71,  382, 1533,   28,  136,   54, 2809,  776,   86,  177,\n",
      "          95,   64, 3371, 1158,  174,  390,   95,   54, 1023,  232,   88,  228,\n",
      "         133,   77,   62,  205,  308,  115,  102,   37,   41,  379,   20,  330,\n",
      "          26,  249,  460,  599, 2662,   92, 2278,  209,   95, 1763,  257,  456,\n",
      "          74, 2384,  241,   79,   74,  204, 2384,  192,   73,   88,  121, 1883,\n",
      "         103,   34,  506,  144,   41, 3901,  209,   78, 1355,   34,  487,  124,\n",
      "         415,  100, 1585,  888,  210,  477, 2804, 1539,  731,  552,   32,  800,\n",
      "         536,   50,  102,   30, 1621,  357, 3087,   30,  329,  186,  209,  837,\n",
      "         428,   62,  828, 2523,  232,   40,  418,  552, 1708,   45,   53,   22,\n",
      "        1922,   60,  452,   95,  452,   99,  993,   53, 2041,  112,   67,   59,\n",
      "         102,   50,  134,  111,  177,   44,  105,   50,  130, 1269,  476,  146,\n",
      "         101,   54,    2])\n",
      "----------------\n",
      "tensor([   1,   30,  126,  545,   66,  374, 1602,  283, 1108,  152,  793,  128,\n",
      "         245,   61,  208, 3056,  552,  635,   99,  819,   42,  558,  283,   65,\n",
      "         232,  204,   32,  346,   75, 3715,  151,  987, 1435,  226,   33,  411,\n",
      "         149,   63,  307,   53, 2695,  102, 1183, 3656,   30,  145,  678,  804,\n",
      "         189,   56,   34,  553,   99,   53, 2975,  112,  131, 1069,   28,  619,\n",
      "         285,   78,  108, 1957,   80,  716, 3833,  177,  100,  118,   48,  941,\n",
      "         158,   23,  519, 1624,  107,   89,  586,  535,  173, 1127,   42,  132,\n",
      "         517,  706,  136,   47,  257,   53,  769,  210, 1795,   19,  402,  232,\n",
      "         586, 1889,  169,  329,   72,  341, 2793,   74,   90, 2307,  177, 1702,\n",
      "        1490,   95,  425, 2803, 2046,  163, 2034,  567,   44,  112,   24,  621,\n",
      "         134, 2288,   53,   57,   26,  166,    2])\n",
      "x tensor([   1,   30,  126,  545,   66,  374, 1602,  283, 1108,  152,  793,  128,\n",
      "         245,   61,  208, 3056,  552,  635,   99,  819,   42,  558,  283,   65,\n",
      "         232,  204,   32,    4,   75, 3715,  151,  987, 1435,  226,   33,  411,\n",
      "         149,   63,  307,   53, 2695,  102, 1183, 3656,   30,  145,  678,  804,\n",
      "         189,   56,   34,  553,   99,   53, 2975,  112,  131, 1069,   28,  619,\n",
      "         285,   78,  108, 1957,   80,  716, 3833,  177,  100,  118,   48,  941,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4])\n",
      "y tensor([   1,   30,  126,  545,   66,  374, 1602,  283, 1108,  152,  793,  128,\n",
      "         245,   61,  208, 3056,  552,  635,   99,  819,   42,  558,  283,   65,\n",
      "         232,  204,   32,  289,   75, 3715,  151,  987, 1435,  226,   33,  411,\n",
      "         149,   63,  307,   53, 2695,  102, 1183, 3656,   30,  145,  678,  804,\n",
      "         189,   56,   34,  553,   99,   53, 2975,  112,  131, 1069,   28,  619,\n",
      "         285,   78,  108, 1957,   80,  716, 3833,  177,  100,  118,   48,  941,\n",
      "        3115,  519, 1624,  107,   89,  586,  535,  173, 1127,   42,  132,  517,\n",
      "         860,  136,   47,  257,   53,  769,  210, 1795,   19,  402,  232,  586,\n",
      "        1889,  403, 3671,  341, 2793,   74,  220,   53, 3667, 1702, 1490, 1767,\n",
      "         215,  108, 2046,  163, 2034,  567,   44,  112,   24,  621,  134, 2288,\n",
      "          53,   57,   26,  166,    2,    3,    3])\n",
      "----------------\n",
      "tensor([   1,    5,   23,   78,  479, 3270,   32,   78, 2018,   67,  158,   70,\n",
      "         886,  232, 1115,  560,   74,   72,  425, 1432,   41,  148,   83,  708,\n",
      "         139,   28,  240,   74,  151,   15,  304,   50,  734,  148, 1425,   32,\n",
      "         307,   33,  382,   11,   53,  173, 3896,   15,  816,  136,   53,   76,\n",
      "          73,  145,  962,  332, 2250,  229,   56,  999, 3812,   76,   35,  205,\n",
      "         932,   19,  192,  290,   70,  694,   47,  866,  601,  203,  205, 2586,\n",
      "          50,  169, 1765, 3431,  462,   66,  541,   90,   74,  987,  247,  519,\n",
      "         203,   25,  496,   33,  752,   42, 1596, 2517,   36, 1036, 3342,  499,\n",
      "         151,   71,  226,   92,  161,   95,   66,  197,  365,   22, 1456,   62,\n",
      "         149,   27,  210,   13,  145,   74,  207,   25,  312, 3273, 1799, 2018,\n",
      "         343,  998,  158,   65,  188, 3422,  371,   55,  317,  450,  188,  102,\n",
      "        2409,  197,   66,   50,  208, 3097,  148,  624,   55,  921,  702,   42,\n",
      "         454,   74,  229,  103,  257,   73, 2010,   34,  561,   74,  257,   74,\n",
      "         716,   51,  156,   93,  190,  106,   51,   36,  989, 1705,  954,   36,\n",
      "         372,  139, 3535,  173,  637,   28,  102,   64,  442,   59,  138,   36,\n",
      "         307,  413, 1907, 1959,  473,   79,  210,  906, 1002,   95,  407,  906,\n",
      "         241, 2190,   28, 2969,  140,   95,   78,  140,  164,   13, 1965,   74,\n",
      "          99,   76,  745,   53,  203,   93,  148,  349,  511,  777,  115, 1556,\n",
      "         212, 1232,  809,  360, 2305,  138,   62,   89,  178,   37,  611,  293,\n",
      "         156,   62,   92,  278,   15,  303,   76,   62,  174, 2027,   33, 1538,\n",
      "          62,  145,  349,  496,   33,  595,   66,   61, 1112,   93,  156,  885,\n",
      "         219,  742,   33,  271,  361,   62,   67,  221,  547,  332,  304,  128,\n",
      "        2690,  905,   61,   92, 2996,   33,  475,   36,  118,   30,  346,   40,\n",
      "         293, 2741,   13, 1260,  349,  460,  840,  367,   93, 2849, 1513,  725,\n",
      "           7,    2])\n",
      "x tensor([   1,    5,   23,   78,  479, 3270,   32,    4,    4,    4,  158,   70,\n",
      "         886,  232, 1115,  560,   74,   72,  425, 1432,   41,  148,   83,  708,\n",
      "         139,   28,  240,   74,  151,   15,  304,   50,  734,  148, 1425,   32,\n",
      "         307,   33,  382,   11,   53,  173, 3896,   15,  816,  136,   53,   76,\n",
      "          73,  145,  962,  332, 2250,  229,   56,  999, 3812,   76,   35,  205,\n",
      "         932,   19,  192,  290,   70,  694,   47,  866,  601,  203,  205, 2586,\n",
      "          50,  169, 1765, 3431,  462,   66,  541,    4,    4,  987,  247,  519,\n",
      "         203,   25,    4,   33,  752,    4,    4, 2517,   36, 1036, 3342,  499,\n",
      "         151,   71,  226,   92,  161,   95,   66,  197,  365,   22, 1456,   62,\n",
      "         149,   27,  210,   13,  145,   74,  207,   25,  312, 3273, 1799, 2018,\n",
      "         343,  998,  158,   65,  188, 3422,  371,   55,    4,  450,  188,  102,\n",
      "        2409,  197,   66,   50,  208, 3097,  148,  624,   55,  921,  702,   42,\n",
      "         454,   74,  229,  103,  257,   73,    4,   34,  561,   74,  257,   74,\n",
      "         716,   51,  156,   93,  190,  106,   51,   36,  989, 1705,  954,   36,\n",
      "         372,  139, 3535,  173,  637,   28,    4,   64,  442,   59,  138,   36,\n",
      "         307,  413, 1907, 1959,  473,   79,  210,  906, 1002,   95,  407,  906,\n",
      "         241, 2190,   28, 2969,  140,   95,   78,  140,  164,   13, 1965,   74,\n",
      "          99,   76,    4,   53,  203,   93,  148,  349,  511,  777,  115, 1556,\n",
      "         212, 1232,  809,    4, 2305,    4,   62,   89,  178,   37,  611,  293,\n",
      "         156,   62,   92,  278,   15,  303,   76,   62,    4, 2027,   33, 1538,\n",
      "          62,  145,  349,  496,   33,  595,   66,   61, 1112,   93,  156,  885,\n",
      "         219,    4,    4,  271,  361,   62,   67,  221,  547,  332,  304,  128,\n",
      "        2690,  905,   61,   92, 2996,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4])\n",
      "y tensor([   1,    5,   23,   78,  479, 3270,   32, 2171,  253,   22,  158,   70,\n",
      "         886,  232, 1115,  560,   74,   72,  425, 1432,   41,  148,   83,  708,\n",
      "         139,   28,  240,   74,  151,   15,  304,   50,  734,  148, 1425,   32,\n",
      "         307,   33,  382,   11,   53,  173, 3896,   15,  816,  136,   53,   76,\n",
      "          73,  145,  962,  332, 2250,  229,   56,  999, 3812,   76,   35,  205,\n",
      "         932,   19,  192,  290,   70,  694,   47,  866,  601,  203,  205, 2586,\n",
      "          50,  169, 1765, 3431,  462,   66,  541,   36,  205,  987,  247,  519,\n",
      "         203,   25,  437,   33,  752,  339,  368, 2517,   36, 1036, 3342,  499,\n",
      "         151,   71,  226,   92,  161,   95,   66,  197,  365,   22, 1456,   62,\n",
      "         149,   27,  210,   13,  145,   74,  207,   25,  312, 3273, 1799, 2018,\n",
      "         343,  998,  158,   65,  188, 3422,  371,   55,  383,  450,  188,  102,\n",
      "        2409,  197,   66,   50,  208, 3097,  148,  624,   55,  921,  702,   42,\n",
      "         454,   74,  229,  103,  257,   73,  742,   34,  561,   74,  257,   74,\n",
      "         716,   51,  156,   93,  190,  106,   51,   36,  989, 1705,  954,   36,\n",
      "         372,  139, 3535,  173,  637,   28,   95,   64,  442,   59,  138,   36,\n",
      "         307,  413, 1907, 1959,  473,   79,  210,  906, 1002,   95,  407,  906,\n",
      "         241, 2190,   28, 2969,  140,   95,   78,  140,  164,   13, 1965,   74,\n",
      "          99,   76, 1589,   53,  203,   93,  148,  349,  511,  777,  115, 1556,\n",
      "         212, 1232,  809,  512, 2305,  103,   62,   89,  178,   37,  611,  293,\n",
      "         156,   62,   92,  278,   15,  303,   76,   62,  159, 2027,   33, 1538,\n",
      "          62,  145,  349,  496,   33,  595,   66,   61, 1112,   93,  156,  885,\n",
      "         219,   83,  757,  271,  361,   62,   67,  221,  547,  332,  304,  128,\n",
      "        2690,  905,   61,   92, 2996, 1036,   86, 1765,   30,  346,   40,  293,\n",
      "        2741,   13, 1260,  349,  460,  840,  367,   93, 2849, 1513,  725,    7,\n",
      "           2,    3])\n",
      "----------------\n",
      "tensor([   1,   42,  329,   57,   65,  720,   33,  181,  351, 1587,  876,  102,\n",
      "          65,  651, 1005,    7,    2])\n",
      "x tensor([   1,   42,  329,   57,   65,  720,   33,  181,  351, 1587,  876,  102,\n",
      "          65,  651, 1005,    7,    2])\n",
      "y tensor([   1,   42,  329,   57,   65,  720,   33,  181,  351, 1587,  876,  102,\n",
      "          65,  651, 1005,    7,    2])\n",
      "----------------\n",
      "tensor([   1, 2089,   72,   78,   65,  134,  161,   13,  212,   67,  229,  102,\n",
      "         171,  123, 1281,  178,   55, 2306, 1176,  209,   90,   86,   26,  278,\n",
      "          62,   64,  102,   56,  143,   15,  156,   62,   26,  156,  261, 1025,\n",
      "          26,  332, 2227, 1220,  103,  169, 2684,  203,  906,  549,  169,   53,\n",
      "          36,  169,   74,  372,  231,   37,   93,   29,  158,  457,  177,  270,\n",
      "         211,   36,  171, 1559, 1682, 3739,  112,  241, 2316,   54,   81, 2920,\n",
      "         102, 3780,  132,   27, 2039,  560, 3728, 2249,  276, 1229,  553,   87,\n",
      "         135,   30,  114,  858, 1684, 2914,  151,  515,   41, 3690,   97, 1689,\n",
      "        3506,  316,  247,   40,  163,  238,   87,   41, 1021,  205,  209, 1585,\n",
      "         345,   77,  992,  222,   68,  466,   65,  582,   75, 4017,  181,  235,\n",
      "          90,  292,  454,   61,  256,  166,  145,  103,   62, 3301, 1681,   50,\n",
      "        2047,   13,  200, 3574,  769, 2065,  803,  984,   76, 2152,  762, 3061,\n",
      "        3416,    8,    2])\n",
      "x tensor([   1, 2089,   72,   78,   65,  134,  161,   13,  212,   67,    4,    4,\n",
      "           4,    4, 1281,  178,   55, 2306, 1176,  209,   90,   86,   26,  278,\n",
      "          62,   64,  102,   56,  143,   15,  156,   62,   26,  156,  261, 1025,\n",
      "          26,  332, 2227, 1220,  103,  169, 2684,  203,  906,  549,  169,   53,\n",
      "          36,  169,   74,  372,  231,   37,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "           4,    4,    4,    4,    4, 2914,  151,  515,   41, 3690,   97, 1689,\n",
      "        3506,  316,    4,    4,  163,  238,   87,    4,    4,  205,  209,    4,\n",
      "           4,    4,  992,  222,   68,  466,   65,  582,   75, 4017,  181,  235,\n",
      "          90,  292,  454,   61,  256,  166,  145,  103,   62, 3301, 1681,   50,\n",
      "        2047,   13,  200, 3574,  769, 2065,  803,  984,   76, 2152,  762, 3061,\n",
      "        3416,    8,    2])\n",
      "y tensor([   1, 2089,   72,   78,   65,  134,  161,   13,  212,   67,  136,   36,\n",
      "         114, 2617, 1281,  178,   55, 2306, 1176,  209,   90,   86,   26,  278,\n",
      "          62,   64,  102,   56,  143,   15,  156,   62,   26,  156,  261, 1025,\n",
      "          26,  332, 2227, 1220,  103,  169, 2684,  203,  906,  549,  169,   53,\n",
      "          36,  169,   74,  372,  231,   37, 1219,  158,  457,  177,  270,  116,\n",
      "          36,  171, 1559, 1682, 3739,  112,  241, 2316,   54,   81, 2920,  102,\n",
      "        3780,  132,   27, 2039,  560, 3728, 2249,  276, 1229,  553,   87,  135,\n",
      "          30,  114,  370,   41,  377, 2914,  151,  515,   41, 3690,   97, 1689,\n",
      "        3506,  316,   32,  239,  163,  238,   87,   90,  184,  205,  209,   47,\n",
      "         277, 2880,  992,  222,   68,  466,   65,  582,   75, 4017,  181,  235,\n",
      "          90,  292,  454,   61,  256,  166,  145,  103,   62, 3301, 1681,   50,\n",
      "        2047,   13,  200, 3574,  769, 2065,  803,  984,   76, 2152,  762, 3061,\n",
      "        3416,    8,    2])\n",
      "----------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'input_ids': tensor([[   1,    4, 1004,   67,   36,  726,  528, 1104,  319,  746,  296,   28,\n",
      "           75, 1507,   55,  362,  123,  130,   82,  443,  184, 2063, 2169,  161,\n",
      "           83,  180,    4,    4,  588,  126,  545,   66,  374, 1602,  283, 1108,\n",
      "          152,  645,  215,    4,  678, 2045,  556, 1176,  727,   97,  173,  448,\n",
      "         1227,  486,   48,  220,   65,   20,    4,  268,   27,  283,  104, 1184,\n",
      "           73, 3532,  245,   61,  208, 3056,  552,  635,   99,  819,   42,  558,\n",
      "          283,   65,  232,  204,   32,  289,   75, 3715,  151,  987, 1435,  226,\n",
      "           33,  411,  149, 3654,  494,  163, 1321,   53, 2975,  112,  131, 1069,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4, 1127, 2293,\n",
      "          448, 3462, 3454,  942,  307,   82, 2491,   50, 1431,  116,   28,  347,\n",
      "          220,   95,  366,  637,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,  776,   86,  177,\n",
      "           95,   64, 3371,    4,  174,  390,   95,   54, 1023,  232,   88,  228,\n",
      "          133,   77,   62,  205,  308,  115,  102,   37,   41,  379,   20,  330,\n",
      "           26,  249,  460,  599, 2662,   92,    4,    4,   95, 1763,  257,  456,\n",
      "           74, 2384,  241,   79,   74,  204, 2384,  192,   73,   88,  121, 1883,\n",
      "          103,    4,    4,  144,   41, 3901,  209,   78, 1355,   34,  487,  124,\n",
      "          415,  100, 1585,  888,  210,  477, 2804, 1539,  731,  552,   32,  800,\n",
      "          536,    4,  102,   30, 1621,  357, 3087,   30,  329,  186,  209,  837,\n",
      "          428,   62,  828, 2523,  232,   40,  418,  552, 1708,   45,   53,   22,\n",
      "         1922,   60,  452,   95,  452,   99,  993,   53, 2041,  112,   67,   59,\n",
      "          102,   50,  134,  111,  177,   44,  105,   50,  130, 1269,  476,  146,\n",
      "          101,   54,    2],\n",
      "        [   1,   30,  126,  545,   66,  374, 1602,  283, 1108,  152,  793,  128,\n",
      "          245,   61,  208, 3056,  552,  635,   99,  819,   42,  558,  283,   65,\n",
      "          232,  204,   32,    4,   75, 3715,  151,  987, 1435,  226,   33,  411,\n",
      "          149,   63,  307,   53, 2695,  102, 1183, 3656,   30,  145,  678,  804,\n",
      "          189,   56,   34,  553,   99,   53, 2975,  112,  131, 1069,   28,  619,\n",
      "          285,   78,  108, 1957,   80,  716, 3833,  177,  100,  118,   48,  941,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3]]), 'labels': tensor([[   1,  153, 1004,   67,   36,  726,  528, 1104,  319,  746,  296,   28,\n",
      "           75, 1507,   55,  362,  123,  130,   82,  443,  184, 2063, 2169,  161,\n",
      "           83,  180, 1719,   54,  588,  126,  545,   66,  374, 1602,  283, 1108,\n",
      "          152,  645,  215, 1347,  678, 2045,  556, 1176,  727,   97,  173,  448,\n",
      "         1227,  486,   48,  220,   65,   20,  150,  268,   27,  283,  104, 1184,\n",
      "           73, 3532,  245,   61,  208, 3056,  552,  635,   99,  819,   42,  558,\n",
      "          283,   65,  232,  204,   32,  289,   75, 3715,  151,  987, 1435,  226,\n",
      "           33,  411,  149, 3654,  494,  163, 1321,   53, 2975,  112,  131, 1069,\n",
      "           28,  619,  285,   78,  108, 1957,   80,  716, 3833,  177,  100,  118,\n",
      "           48,  941, 3115,  519, 1624,  107,   89,  586,  535,  173, 1127, 2293,\n",
      "          448, 3462, 3454,  942,  307,   82, 2491,   50, 1431,  116,   28,  347,\n",
      "          220,   95,  366,  637,   35,  136,   47,  257,   53,  769,  210, 1795,\n",
      "           19,  402,  232,  586, 1889,  403, 3671,  341, 2793,   74,  220,   53,\n",
      "         3667, 1702, 1490, 1767,  215,  108, 2046,  163, 2034,  567,   44,  112,\n",
      "           24,  621,  134, 2288,   53,   57,   26,  166,  348,  281,  177,   89,\n",
      "           26,   61,  822,  567,   32,  241,  261, 2293,  144,   54, 1403,  177,\n",
      "          123,   36,   71,  382, 1533,   28,  136,   54, 2809,  776,   86,  177,\n",
      "           95,   64, 3371, 1158,  174,  390,   95,   54, 1023,  232,   88,  228,\n",
      "          133,   77,   62,  205,  308,  115,  102,   37,   41,  379,   20,  330,\n",
      "           26,  249,  460,  599, 2662,   92, 2278,  209,   95, 1763,  257,  456,\n",
      "           74, 2384,  241,   79,   74,  204, 2384,  192,   73,   88,  121, 1883,\n",
      "          103,   34,  506,  144,   41, 3901,  209,   78, 1355,   34,  487,  124,\n",
      "          415,  100, 1585,  888,  210,  477, 2804, 1539,  731,  552,   32,  800,\n",
      "          536,   50,  102,   30, 1621,  357, 3087,   30,  329,  186,  209,  837,\n",
      "          428,   62,  828, 2523,  232,   40,  418,  552, 1708,   45,   53,   22,\n",
      "         1922,   60,  452,   95,  452,   99,  993,   53, 2041,  112,   67,   59,\n",
      "          102,   50,  134,  111,  177,   44,  105,   50,  130, 1269,  476,  146,\n",
      "          101,   54,    2],\n",
      "        [   1,   30,  126,  545,   66,  374, 1602,  283, 1108,  152,  793,  128,\n",
      "          245,   61,  208, 3056,  552,  635,   99,  819,   42,  558,  283,   65,\n",
      "          232,  204,   32,  289,   75, 3715,  151,  987, 1435,  226,   33,  411,\n",
      "          149,   63,  307,   53, 2695,  102, 1183, 3656,   30,  145,  678,  804,\n",
      "          189,   56,   34,  553,   99,   53, 2975,  112,  131, 1069,   28,  619,\n",
      "          285,   78,  108, 1957,   80,  716, 3833,  177,  100,  118,   48,  941,\n",
      "         3115,  519, 1624,  107,   89,  586,  535,  173, 1127,   42,  132,  517,\n",
      "          860,  136,   47,  257,   53,  769,  210, 1795,   19,  402,  232,  586,\n",
      "         1889,  403, 3671,  341, 2793,   74,  220,   53, 3667, 1702, 1490, 1767,\n",
      "          215,  108, 2046,  163, 2034,  567,   44,  112,   24,  621,  134, 2288,\n",
      "           53,   57,   26,  166,    2,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3]])}\n",
      "{'input_ids': tensor([[   1,    4, 1004,   67,   36,  726,  528, 1104,  319,  746,  296,   28,\n",
      "           75, 1507,   55,  362,  123,  130,   82,  443,  184, 2063, 2169,  161,\n",
      "           83,  180,    4,    4,  588,  126,  545,   66,  374, 1602,  283, 1108,\n",
      "          152,  645,  215,    4,  678, 2045,  556, 1176,  727,   97,  173,  448,\n",
      "         1227,  486,   48,  220,   65,   20,    4,  268,   27,  283,  104, 1184,\n",
      "           73, 3532,  245,   61,  208, 3056,  552,  635,   99,  819,   42,  558,\n",
      "          283,   65,  232,  204,   32,  289,   75, 3715,  151,  987, 1435,  226,\n",
      "           33,  411,  149, 3654,  494,  163, 1321,   53, 2975,  112,  131, 1069,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4, 1127, 2293,\n",
      "          448, 3462, 3454,  942,  307,   82, 2491,   50, 1431,  116,   28,  347,\n",
      "          220,   95,  366,  637,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,  776,   86,  177,\n",
      "           95,   64, 3371,    4,  174,  390,   95,   54, 1023,  232,   88,  228,\n",
      "          133,   77,   62,  205,  308,  115,  102,   37,   41,  379,   20,  330,\n",
      "           26,  249,  460,  599, 2662,   92,    4,    4,   95, 1763,  257,  456,\n",
      "           74, 2384,  241,   79,   74,  204, 2384,  192,   73,   88,  121, 1883,\n",
      "          103,    4,    4,  144,   41, 3901,  209,   78, 1355,   34,  487,  124,\n",
      "          415,  100, 1585,  888,  210,  477, 2804, 1539,  731,  552,   32,  800,\n",
      "          536,    4,  102,   30, 1621,  357, 3087,   30,  329,  186,  209,  837,\n",
      "          428,   62,  828, 2523,  232,   40,  418,  552, 1708,   45,   53,   22,\n",
      "         1922,   60,  452,   95,  452,   99,  993,   53, 2041,  112,   67,   59,\n",
      "          102,   50,  134,  111,  177,   44,  105,   50,  130, 1269,  476,  146,\n",
      "          101,   54,    2],\n",
      "        [   1,   30,  126,  545,   66,  374, 1602,  283, 1108,  152,  793,  128,\n",
      "          245,   61,  208, 3056,  552,  635,   99,  819,   42,  558,  283,   65,\n",
      "          232,  204,   32,    4,   75, 3715,  151,  987, 1435,  226,   33,  411,\n",
      "          149,   63,  307,   53, 2695,  102, 1183, 3656,   30,  145,  678,  804,\n",
      "          189,   56,   34,  553,   99,   53, 2975,  112,  131, 1069,   28,  619,\n",
      "          285,   78,  108, 1957,   80,  716, 3833,  177,  100,  118,   48,  941,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3]]), 'labels': tensor([[   1,  153, 1004,   67,   36,  726,  528, 1104,  319,  746,  296,   28,\n",
      "           75, 1507,   55,  362,  123,  130,   82,  443,  184, 2063, 2169,  161,\n",
      "           83,  180, 1719,   54,  588,  126,  545,   66,  374, 1602,  283, 1108,\n",
      "          152,  645,  215, 1347,  678, 2045,  556, 1176,  727,   97,  173,  448,\n",
      "         1227,  486,   48,  220,   65,   20,  150,  268,   27,  283,  104, 1184,\n",
      "           73, 3532,  245,   61,  208, 3056,  552,  635,   99,  819,   42,  558,\n",
      "          283,   65,  232,  204,   32,  289,   75, 3715,  151,  987, 1435,  226,\n",
      "           33,  411,  149, 3654,  494,  163, 1321,   53, 2975,  112,  131, 1069,\n",
      "           28,  619,  285,   78,  108, 1957,   80,  716, 3833,  177,  100,  118,\n",
      "           48,  941, 3115,  519, 1624,  107,   89,  586,  535,  173, 1127, 2293,\n",
      "          448, 3462, 3454,  942,  307,   82, 2491,   50, 1431,  116,   28,  347,\n",
      "          220,   95,  366,  637,   35,  136,   47,  257,   53,  769,  210, 1795,\n",
      "           19,  402,  232,  586, 1889,  403, 3671,  341, 2793,   74,  220,   53,\n",
      "         3667, 1702, 1490, 1767,  215,  108, 2046,  163, 2034,  567,   44,  112,\n",
      "           24,  621,  134, 2288,   53,   57,   26,  166,  348,  281,  177,   89,\n",
      "           26,   61,  822,  567,   32,  241,  261, 2293,  144,   54, 1403,  177,\n",
      "          123,   36,   71,  382, 1533,   28,  136,   54, 2809,  776,   86,  177,\n",
      "           95,   64, 3371, 1158,  174,  390,   95,   54, 1023,  232,   88,  228,\n",
      "          133,   77,   62,  205,  308,  115,  102,   37,   41,  379,   20,  330,\n",
      "           26,  249,  460,  599, 2662,   92, 2278,  209,   95, 1763,  257,  456,\n",
      "           74, 2384,  241,   79,   74,  204, 2384,  192,   73,   88,  121, 1883,\n",
      "          103,   34,  506,  144,   41, 3901,  209,   78, 1355,   34,  487,  124,\n",
      "          415,  100, 1585,  888,  210,  477, 2804, 1539,  731,  552,   32,  800,\n",
      "          536,   50,  102,   30, 1621,  357, 3087,   30,  329,  186,  209,  837,\n",
      "          428,   62,  828, 2523,  232,   40,  418,  552, 1708,   45,   53,   22,\n",
      "         1922,   60,  452,   95,  452,   99,  993,   53, 2041,  112,   67,   59,\n",
      "          102,   50,  134,  111,  177,   44,  105,   50,  130, 1269,  476,  146,\n",
      "          101,   54,    2],\n",
      "        [   1,   30,  126,  545,   66,  374, 1602,  283, 1108,  152,  793,  128,\n",
      "          245,   61,  208, 3056,  552,  635,   99,  819,   42,  558,  283,   65,\n",
      "          232,  204,   32,  289,   75, 3715,  151,  987, 1435,  226,   33,  411,\n",
      "          149,   63,  307,   53, 2695,  102, 1183, 3656,   30,  145,  678,  804,\n",
      "          189,   56,   34,  553,   99,   53, 2975,  112,  131, 1069,   28,  619,\n",
      "          285,   78,  108, 1957,   80,  716, 3833,  177,  100,  118,   48,  941,\n",
      "         3115,  519, 1624,  107,   89,  586,  535,  173, 1127,   42,  132,  517,\n",
      "          860,  136,   47,  257,   53,  769,  210, 1795,   19,  402,  232,  586,\n",
      "         1889,  403, 3671,  341, 2793,   74,  220,   53, 3667, 1702, 1490, 1767,\n",
      "          215,  108, 2046,  163, 2034,  567,   44,  112,   24,  621,  134, 2288,\n",
      "           53,   57,   26,  166,    2,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3]])}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'input_ids': tensor([[   1,    5,   23,   78,  479, 3270,   32,    4,    4,    4,  158,   70,\n",
      "          886,  232, 1115,  560,   74,   72,  425, 1432,   41,  148,   83,  708,\n",
      "          139,   28,  240,   74,  151,   15,  304,   50,  734,  148, 1425,   32,\n",
      "          307,   33,  382,   11,   53,  173, 3896,   15,  816,  136,   53,   76,\n",
      "           73,  145,  962,  332, 2250,  229,   56,  999, 3812,   76,   35,  205,\n",
      "          932,   19,  192,  290,   70,  694,   47,  866,  601,  203,  205, 2586,\n",
      "           50,  169, 1765, 3431,  462,   66,  541,    4,    4,  987,  247,  519,\n",
      "          203,   25,    4,   33,  752,    4,    4, 2517,   36, 1036, 3342,  499,\n",
      "          151,   71,  226,   92,  161,   95,   66,  197,  365,   22, 1456,   62,\n",
      "          149,   27,  210,   13,  145,   74,  207,   25,  312, 3273, 1799, 2018,\n",
      "          343,  998,  158,   65,  188, 3422,  371,   55,    4,  450,  188,  102,\n",
      "         2409,  197,   66,   50,  208, 3097,  148,  624,   55,  921,  702,   42,\n",
      "          454,   74,  229,  103,  257,   73,    4,   34,  561,   74,  257,   74,\n",
      "          716,   51,  156,   93,  190,  106,   51,   36,  989, 1705,  954,   36,\n",
      "          372,  139, 3535,  173,  637,   28,    4,   64,  442,   59,  138,   36,\n",
      "          307,  413, 1907, 1959,  473,   79,  210,  906, 1002,   95,  407,  906,\n",
      "          241, 2190,   28, 2969,  140,   95,   78,  140,  164,   13, 1965,   74,\n",
      "           99,   76,    4,   53,  203,   93,  148,  349,  511,  777,  115, 1556,\n",
      "          212, 1232,  809,    4, 2305,    4,   62,   89,  178,   37,  611,  293,\n",
      "          156,   62,   92,  278,   15,  303,   76,   62,    4, 2027,   33, 1538,\n",
      "           62,  145,  349,  496,   33,  595,   66,   61, 1112,   93,  156,  885,\n",
      "          219,    4,    4,  271,  361,   62,   67,  221,  547,  332,  304,  128,\n",
      "         2690,  905,   61,   92, 2996,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4],\n",
      "        [   1,   42,  329,   57,   65,  720,   33,  181,  351, 1587,  876,  102,\n",
      "           65,  651, 1005,    7,    2,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3]]), 'labels': tensor([[   1,    5,   23,   78,  479, 3270,   32, 2171,  253,   22,  158,   70,\n",
      "          886,  232, 1115,  560,   74,   72,  425, 1432,   41,  148,   83,  708,\n",
      "          139,   28,  240,   74,  151,   15,  304,   50,  734,  148, 1425,   32,\n",
      "          307,   33,  382,   11,   53,  173, 3896,   15,  816,  136,   53,   76,\n",
      "           73,  145,  962,  332, 2250,  229,   56,  999, 3812,   76,   35,  205,\n",
      "          932,   19,  192,  290,   70,  694,   47,  866,  601,  203,  205, 2586,\n",
      "           50,  169, 1765, 3431,  462,   66,  541,   36,  205,  987,  247,  519,\n",
      "          203,   25,  437,   33,  752,  339,  368, 2517,   36, 1036, 3342,  499,\n",
      "          151,   71,  226,   92,  161,   95,   66,  197,  365,   22, 1456,   62,\n",
      "          149,   27,  210,   13,  145,   74,  207,   25,  312, 3273, 1799, 2018,\n",
      "          343,  998,  158,   65,  188, 3422,  371,   55,  383,  450,  188,  102,\n",
      "         2409,  197,   66,   50,  208, 3097,  148,  624,   55,  921,  702,   42,\n",
      "          454,   74,  229,  103,  257,   73,  742,   34,  561,   74,  257,   74,\n",
      "          716,   51,  156,   93,  190,  106,   51,   36,  989, 1705,  954,   36,\n",
      "          372,  139, 3535,  173,  637,   28,   95,   64,  442,   59,  138,   36,\n",
      "          307,  413, 1907, 1959,  473,   79,  210,  906, 1002,   95,  407,  906,\n",
      "          241, 2190,   28, 2969,  140,   95,   78,  140,  164,   13, 1965,   74,\n",
      "           99,   76, 1589,   53,  203,   93,  148,  349,  511,  777,  115, 1556,\n",
      "          212, 1232,  809,  512, 2305,  103,   62,   89,  178,   37,  611,  293,\n",
      "          156,   62,   92,  278,   15,  303,   76,   62,  159, 2027,   33, 1538,\n",
      "           62,  145,  349,  496,   33,  595,   66,   61, 1112,   93,  156,  885,\n",
      "          219,   83,  757,  271,  361,   62,   67,  221,  547,  332,  304,  128,\n",
      "         2690,  905,   61,   92, 2996, 1036,   86, 1765,   30,  346,   40,  293,\n",
      "         2741,   13, 1260,  349,  460,  840,  367,   93, 2849, 1513,  725,    7,\n",
      "            2,    3],\n",
      "        [   1,   42,  329,   57,   65,  720,   33,  181,  351, 1587,  876,  102,\n",
      "           65,  651, 1005,    7,    2,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3]])}\n",
      "{'input_ids': tensor([[   1,    5,   23,   78,  479, 3270,   32,    4,    4,    4,  158,   70,\n",
      "          886,  232, 1115,  560,   74,   72,  425, 1432,   41,  148,   83,  708,\n",
      "          139,   28,  240,   74,  151,   15,  304,   50,  734,  148, 1425,   32,\n",
      "          307,   33,  382,   11,   53,  173, 3896,   15,  816,  136,   53,   76,\n",
      "           73,  145,  962,  332, 2250,  229,   56,  999, 3812,   76,   35,  205,\n",
      "          932,   19,  192,  290,   70,  694,   47,  866,  601,  203,  205, 2586,\n",
      "           50,  169, 1765, 3431,  462,   66,  541,    4,    4,  987,  247,  519,\n",
      "          203,   25,    4,   33,  752,    4,    4, 2517,   36, 1036, 3342,  499,\n",
      "          151,   71,  226,   92,  161,   95,   66,  197,  365,   22, 1456,   62,\n",
      "          149,   27,  210,   13,  145,   74,  207,   25,  312, 3273, 1799, 2018,\n",
      "          343,  998,  158,   65,  188, 3422,  371,   55,    4,  450,  188,  102,\n",
      "         2409,  197,   66,   50,  208, 3097,  148,  624,   55,  921,  702,   42,\n",
      "          454,   74,  229,  103,  257,   73,    4,   34,  561,   74,  257,   74,\n",
      "          716,   51,  156,   93,  190,  106,   51,   36,  989, 1705,  954,   36,\n",
      "          372,  139, 3535,  173,  637,   28,    4,   64,  442,   59,  138,   36,\n",
      "          307,  413, 1907, 1959,  473,   79,  210,  906, 1002,   95,  407,  906,\n",
      "          241, 2190,   28, 2969,  140,   95,   78,  140,  164,   13, 1965,   74,\n",
      "           99,   76,    4,   53,  203,   93,  148,  349,  511,  777,  115, 1556,\n",
      "          212, 1232,  809,    4, 2305,    4,   62,   89,  178,   37,  611,  293,\n",
      "          156,   62,   92,  278,   15,  303,   76,   62,    4, 2027,   33, 1538,\n",
      "           62,  145,  349,  496,   33,  595,   66,   61, 1112,   93,  156,  885,\n",
      "          219,    4,    4,  271,  361,   62,   67,  221,  547,  332,  304,  128,\n",
      "         2690,  905,   61,   92, 2996,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4],\n",
      "        [   1,   42,  329,   57,   65,  720,   33,  181,  351, 1587,  876,  102,\n",
      "           65,  651, 1005,    7,    2,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3]]), 'labels': tensor([[   1,    5,   23,   78,  479, 3270,   32, 2171,  253,   22,  158,   70,\n",
      "          886,  232, 1115,  560,   74,   72,  425, 1432,   41,  148,   83,  708,\n",
      "          139,   28,  240,   74,  151,   15,  304,   50,  734,  148, 1425,   32,\n",
      "          307,   33,  382,   11,   53,  173, 3896,   15,  816,  136,   53,   76,\n",
      "           73,  145,  962,  332, 2250,  229,   56,  999, 3812,   76,   35,  205,\n",
      "          932,   19,  192,  290,   70,  694,   47,  866,  601,  203,  205, 2586,\n",
      "           50,  169, 1765, 3431,  462,   66,  541,   36,  205,  987,  247,  519,\n",
      "          203,   25,  437,   33,  752,  339,  368, 2517,   36, 1036, 3342,  499,\n",
      "          151,   71,  226,   92,  161,   95,   66,  197,  365,   22, 1456,   62,\n",
      "          149,   27,  210,   13,  145,   74,  207,   25,  312, 3273, 1799, 2018,\n",
      "          343,  998,  158,   65,  188, 3422,  371,   55,  383,  450,  188,  102,\n",
      "         2409,  197,   66,   50,  208, 3097,  148,  624,   55,  921,  702,   42,\n",
      "          454,   74,  229,  103,  257,   73,  742,   34,  561,   74,  257,   74,\n",
      "          716,   51,  156,   93,  190,  106,   51,   36,  989, 1705,  954,   36,\n",
      "          372,  139, 3535,  173,  637,   28,   95,   64,  442,   59,  138,   36,\n",
      "          307,  413, 1907, 1959,  473,   79,  210,  906, 1002,   95,  407,  906,\n",
      "          241, 2190,   28, 2969,  140,   95,   78,  140,  164,   13, 1965,   74,\n",
      "           99,   76, 1589,   53,  203,   93,  148,  349,  511,  777,  115, 1556,\n",
      "          212, 1232,  809,  512, 2305,  103,   62,   89,  178,   37,  611,  293,\n",
      "          156,   62,   92,  278,   15,  303,   76,   62,  159, 2027,   33, 1538,\n",
      "           62,  145,  349,  496,   33,  595,   66,   61, 1112,   93,  156,  885,\n",
      "          219,   83,  757,  271,  361,   62,   67,  221,  547,  332,  304,  128,\n",
      "         2690,  905,   61,   92, 2996, 1036,   86, 1765,   30,  346,   40,  293,\n",
      "         2741,   13, 1260,  349,  460,  840,  367,   93, 2849, 1513,  725,    7,\n",
      "            2,    3],\n",
      "        [   1,   42,  329,   57,   65,  720,   33,  181,  351, 1587,  876,  102,\n",
      "           65,  651, 1005,    7,    2,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "            3,    3]])}\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'input_ids': tensor([[   1, 2089,   72,   78,   65,  134,  161,   13,  212,   67,    4,    4,\n",
      "            4,    4, 1281,  178,   55, 2306, 1176,  209,   90,   86,   26,  278,\n",
      "           62,   64,  102,   56,  143,   15,  156,   62,   26,  156,  261, 1025,\n",
      "           26,  332, 2227, 1220,  103,  169, 2684,  203,  906,  549,  169,   53,\n",
      "           36,  169,   74,  372,  231,   37,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4, 2914,  151,  515,   41, 3690,   97, 1689,\n",
      "         3506,  316,    4,    4,  163,  238,   87,    4,    4,  205,  209,    4,\n",
      "            4,    4,  992,  222,   68,  466,   65,  582,   75, 4017,  181,  235,\n",
      "           90,  292,  454,   61,  256,  166,  145,  103,   62, 3301, 1681,   50,\n",
      "         2047,   13,  200, 3574,  769, 2065,  803,  984,   76, 2152,  762, 3061,\n",
      "         3416,    8,    2]]), 'labels': tensor([[   1, 2089,   72,   78,   65,  134,  161,   13,  212,   67,  136,   36,\n",
      "          114, 2617, 1281,  178,   55, 2306, 1176,  209,   90,   86,   26,  278,\n",
      "           62,   64,  102,   56,  143,   15,  156,   62,   26,  156,  261, 1025,\n",
      "           26,  332, 2227, 1220,  103,  169, 2684,  203,  906,  549,  169,   53,\n",
      "           36,  169,   74,  372,  231,   37, 1219,  158,  457,  177,  270,  116,\n",
      "           36,  171, 1559, 1682, 3739,  112,  241, 2316,   54,   81, 2920,  102,\n",
      "         3780,  132,   27, 2039,  560, 3728, 2249,  276, 1229,  553,   87,  135,\n",
      "           30,  114,  370,   41,  377, 2914,  151,  515,   41, 3690,   97, 1689,\n",
      "         3506,  316,   32,  239,  163,  238,   87,   90,  184,  205,  209,   47,\n",
      "          277, 2880,  992,  222,   68,  466,   65,  582,   75, 4017,  181,  235,\n",
      "           90,  292,  454,   61,  256,  166,  145,  103,   62, 3301, 1681,   50,\n",
      "         2047,   13,  200, 3574,  769, 2065,  803,  984,   76, 2152,  762, 3061,\n",
      "         3416,    8,    2]])}\n",
      "{'input_ids': tensor([[   1, 2089,   72,   78,   65,  134,  161,   13,  212,   67,    4,    4,\n",
      "            4,    4, 1281,  178,   55, 2306, 1176,  209,   90,   86,   26,  278,\n",
      "           62,   64,  102,   56,  143,   15,  156,   62,   26,  156,  261, 1025,\n",
      "           26,  332, 2227, 1220,  103,  169, 2684,  203,  906,  549,  169,   53,\n",
      "           36,  169,   74,  372,  231,   37,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,    4,\n",
      "            4,    4,    4,    4,    4, 2914,  151,  515,   41, 3690,   97, 1689,\n",
      "         3506,  316,    4,    4,  163,  238,   87,    4,    4,  205,  209,    4,\n",
      "            4,    4,  992,  222,   68,  466,   65,  582,   75, 4017,  181,  235,\n",
      "           90,  292,  454,   61,  256,  166,  145,  103,   62, 3301, 1681,   50,\n",
      "         2047,   13,  200, 3574,  769, 2065,  803,  984,   76, 2152,  762, 3061,\n",
      "         3416,    8,    2]]), 'labels': tensor([[   1, 2089,   72,   78,   65,  134,  161,   13,  212,   67,  136,   36,\n",
      "          114, 2617, 1281,  178,   55, 2306, 1176,  209,   90,   86,   26,  278,\n",
      "           62,   64,  102,   56,  143,   15,  156,   62,   26,  156,  261, 1025,\n",
      "           26,  332, 2227, 1220,  103,  169, 2684,  203,  906,  549,  169,   53,\n",
      "           36,  169,   74,  372,  231,   37, 1219,  158,  457,  177,  270,  116,\n",
      "           36,  171, 1559, 1682, 3739,  112,  241, 2316,   54,   81, 2920,  102,\n",
      "         3780,  132,   27, 2039,  560, 3728, 2249,  276, 1229,  553,   87,  135,\n",
      "           30,  114,  370,   41,  377, 2914,  151,  515,   41, 3690,   97, 1689,\n",
      "         3506,  316,   32,  239,  163,  238,   87,   90,  184,  205,  209,   47,\n",
      "          277, 2880,  992,  222,   68,  466,   65,  582,   75, 4017,  181,  235,\n",
      "           90,  292,  454,   61,  256,  166,  145,  103,   62, 3301, 1681,   50,\n",
      "         2047,   13,  200, 3574,  769, 2065,  803,  984,   76, 2152,  762, 3061,\n",
      "         3416,    8,    2]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29701/3242515884.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(self.sequences[idx]), 'labels': torch.tensor(self.tokens_labels[idx])}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from Bio import SeqIO\n",
    "import random\n",
    "MAX_LEN = 512\n",
    "MASK_TOKEN = 4 # [MASK] token\n",
    "PAD_TOKEN = 3 # [PAD] token\n",
    "ENDING_TOKEN = 2 # [SEP] token\n",
    "\n",
    "def insert_single_replacement(seq, mutation_rate):\n",
    "    new_seq = []\n",
    "    mutation_added = False\n",
    "    for c in seq:\n",
    "        if mutation_added == False and random.random() < mutation_rate:\n",
    "            choice_str = list({'A', 'C', 'G', 'T'} - {c})\n",
    "            new_seq.append(random.choice(choice_str))\n",
    "        else:\n",
    "            new_seq.append(c)\n",
    "    return ''.join(new_seq)\n",
    "\n",
    "\n",
    "def get_onehot_for_first_missmatch(seq1, seq2):\n",
    "    for index, (a, b) in enumerate(zip(seq1, seq2)):\n",
    "        if a != b:\n",
    "            return [1 if i == index else 0 for i in range(len(seq1))]\n",
    "    return [0] * len(seq1)\n",
    "\n",
    "def pad_sequences(seq1, seq2, max_len=MAX_LEN):\n",
    "    max_len = max(len(seq1), len(seq2))\n",
    "    if seq1.shape[0] < max_len:\n",
    "        # seq1[-1] = PAD_TOKEN\n",
    "        pad_vector = torch.ones(max_len - len(seq1), dtype=torch.long) * PAD_TOKEN\n",
    "        seq1 = torch.cat((seq1, pad_vector))\n",
    "        # seq1[-1] = ENDING_TOKEN\n",
    "\n",
    "\n",
    "    elif seq2.shape[0] < max_len:\n",
    "        # seq2[-1] = PAD_TOKEN\n",
    "        pad_vector = torch.ones(max_len - len(seq2), dtype=torch.long) * PAD_TOKEN\n",
    "        seq2 = torch.cat((seq2, pad_vector))\n",
    "        # seq2[-1] = ENDING_TOKEN\n",
    "    \n",
    "    assert seq1.shape == seq2.shape\n",
    "    return seq1, seq2\n",
    "\n",
    "\n",
    "def compare_two_sequences(seq1, seq2):\n",
    "    return seq1 != seq2\n",
    "\n",
    "\n",
    "def mask_sequence(seq, mask_vector):\n",
    "    seq[mask_vector] = MASK_TOKEN # [MASK] token\n",
    "    return seq\n",
    "\n",
    "\n",
    "class MutationDetectionDataset(Dataset):\n",
    "\n",
    "    def __init__(self, fasta_m, fasta_t, tokenization_f, replacement_flag=False, mutation_rate=0.01, verbose=False):\n",
    " \n",
    "        zipped_fasta_lines = zip(SeqIO.parse(fasta_m, \"fasta\"), SeqIO.parse(fasta_t, \"fasta\"))\n",
    "        self.sequences = []\n",
    "        self.tokens_labels = []\n",
    "        self.mutations = []\n",
    "        for record_m, record_t in zipped_fasta_lines:\n",
    "            if replacement_flag :\n",
    "                x = insert_single_replacement(record_m.seq, mutation_rate=mutation_rate)\n",
    "            else:\n",
    "                x = record_m.seq\n",
    "            tokenized_x = tokenization_f(str(x), padding=False, truncation=True, max_length=MAX_LEN, return_tensors='pt')['input_ids'].squeeze(0)\n",
    "            print(tokenized_x)\n",
    "            tokenized_y = tokenization_f(str(record_t.seq), padding=False, truncation=True, max_length=MAX_LEN, return_tensors='pt')['input_ids'].squeeze(0)\n",
    "            tokenized_x, tokenized_y = pad_sequences(tokenized_x, tokenized_y)\n",
    "            mask_vector = compare_two_sequences(tokenized_x, tokenized_y)\n",
    "            tokenized_x = mask_sequence(tokenized_x, mask_vector)\n",
    "            self.sequences.append(tokenized_x)\n",
    "            self.tokens_labels.append(tokenized_y)\n",
    "            if verbose:\n",
    "                print('x', tokenized_x)\n",
    "                print('y', tokenized_y)\n",
    "                # print(tokenized_x.shape == tokenized_y.shape)\n",
    "                \n",
    "                # print(tokenized_y.dtype)\n",
    "                # print('compare', mask_vector)\n",
    "                print('----------------')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert self.sequences[idx].shape == self.tokens_labels[idx].shape\n",
    "        return {'input_ids': torch.tensor(self.sequences[idx]), 'labels': torch.tensor(self.tokens_labels[idx])}\n",
    "    \n",
    "\n",
    "mutation_dataset = MutationDetectionDataset(fasta_m, fasta_t, tokenizer, verbose=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True, max_length=MAX_LEN)\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    max_len = max([len(t) for t in input_ids])\n",
    "\n",
    "    input_ids = [torch.cat((t, torch.ones(max_len - len(t), dtype=torch.long) * PAD_TOKEN)) for t in input_ids]\n",
    "    labels = [torch.cat((t, torch.ones(max_len - len(t), dtype=torch.long) * PAD_TOKEN)) for t in labels]\n",
    "\n",
    "    return {'input_ids': torch.stack(input_ids), 'labels': torch.stack(labels)}\n",
    "\n",
    "dataloader = DataLoader(mutation_dataset, batch_size=2, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print('-' * 100)\n",
    "    print(batch)\n",
    "    print(batch)\n",
    "    # print('-' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29701/3242515884.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {'input_ids': torch.tensor(self.sequences[idx]), 'labels': torch.tensor(self.tokens_labels[idx])}\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/mutation_correction_env_1/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:718\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 718\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    725\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 327 at dim 1 (got 127)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(mutation_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, collate_fn\u001b[38;5;241m=\u001b[39mdata_collator, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Iterate over the DataLoader\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mutation_correction_env_1/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/mutation_correction_env_1/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/mutation_correction_env_1/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:61\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mutation_correction_env_1/lib/python3.10/site-packages/transformers/data/data_collator.py:249\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 249\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m    257\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/mutation_correction_env_1/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3045\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3042\u001b[0m             batch_outputs[key] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3043\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[0;32m-> 3045\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mutation_correction_env_1/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:211\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    207\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mutation_correction_env_1/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:734\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    729\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    730\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    731\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    732\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    733\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 734\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    735\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    736\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    737\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    738\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    739\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "dataloader = DataLoader(mutation_dataset, batch_size=2, collate_fn=data_collator, shuffle=False)\n",
    "# Iterate over the DataLoader\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   1,    4, 1004,   67,   36,  726,  528, 1104,  319,  746,  296,   28,\n",
      "           75, 1507,   55,  362,  123,  130,   82,  443,  184, 2063, 2169,  161,\n",
      "           83,  180,    4,    4,  588,  126,  545,    2],\n",
      "        [   1,   30,  126,  545,   66,  374, 1602,  283, 1108,  152,  793,  128,\n",
      "          245,   61,  208, 3056,  552,  635,   99,  819,   42,  558,  283,   65,\n",
      "          232,  204,   32,    4,   75, 3715,  151,    2]]), 'labels': tensor([[   1,  153, 1004,   67,   36,  726,  528, 1104,  319,  746,  296,   28,\n",
      "           75, 1507,   55,  362,  123,  130,   82,  443,  184, 2063, 2169,  161,\n",
      "           83,  180, 1719,   54,  588,  126,  545,    2],\n",
      "        [   1,   30,  126,  545,   66,  374, 1602,  283, 1108,  152,  793,  128,\n",
      "          245,   61,  208, 3056,  552,  635,   99,  819,   42,  558,  283,   65,\n",
      "          232,  204,   32,  289,   75, 3715,  151,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/mutation_correction_env_1/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:718\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 718\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    725\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 32 at dim 1 (got 17)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Iterate over the DataLoader\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mutation_correction_env_1/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/mutation_correction_env_1/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/mutation_correction_env_1/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:61\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mutation_correction_env_1/lib/python3.10/site-packages/transformers/data/data_collator.py:249\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 249\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m    257\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/mutation_correction_env_1/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3045\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3042\u001b[0m             batch_outputs[key] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3043\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[0;32m-> 3045\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mutation_correction_env_1/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:211\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    207\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mutation_correction_env_1/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:734\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    729\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    730\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    731\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    732\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    733\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 734\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    735\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    736\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    737\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    738\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    739\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "# Iterate over the DataLoader\n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<data_handling_for_MLM.MutationDetectionDataset at 0x7f3e02406920>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutation_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "We are using [DNABERT2](https://github.com/MAGICS-LAB/DNABERT_2/tree/main?tab=readme-ov-file#1-introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(4096, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=4096, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mutation_sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
